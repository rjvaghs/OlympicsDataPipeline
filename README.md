# OlympicsDataPipeline
End-to-end Olympics data pipeline using Azure Data Factory, Azure Synapse Analytics and PowerBI

## ğŸ“Œ Project Overview
This project ingests **Olympics data from Kaggle** into **Azure Data Lake** and processes it using **Azure Synapse**. The pipeline includes:
- **Azure Function Blob Trigger**: Automatically unzips CSV files when uploaded.
- **Dataflow & Apache Spark**: Cleans and transforms the data.
- **Dedicated SQL Pool**: Stores processed data in staging and final tables.
- **Power BI**: Visualizes insights from the data.

## ğŸ“ Project Structure
```
ğŸ“‚ olympics-azure-project
â”œâ”€â”€ ğŸ“œ README.md  # Project documentation
â”œâ”€â”€ ğŸ“‚ azure-function
â”‚   â”œâ”€â”€ blob_trigger_function.py  # Azure function to unzip files
â”‚   â”œâ”€â”€ requirements.txt  # Dependencies for the function
â”œâ”€â”€ ğŸ“‚ synapse-pipeline
â”‚   â”œâ”€â”€ ingest_olympics_pipeline.json  # Synapse pipeline JSON
â”‚   â”œâ”€â”€ transformations_notebook.ipynb  # Apache Spark notebook for transformation
â”œâ”€â”€ ğŸ“‚ sql-scripts
â”‚   â”œâ”€â”€ create_staging_table.sql  # SQL script for staging table
â”‚   â”œâ”€â”€ insert_into_target.sql  # SQL script for final table
â”œâ”€â”€ ğŸ“‚ powerbi-report
â”‚   â”œâ”€â”€ olympics_dashboard.pbix  # Power BI visualization
â””â”€â”€ ğŸ“‚ data-samples
    â”œâ”€â”€ sample_olympics_data.zip  # Sample ZIP file with CSVs
```

## Data Architecture
![Data Architecture](https://github.com/user-attachments/assets/aad61e0d-2f69-463e-bb8f-6ea29deb951d)


## ğŸš€ **End-to-End Process Flow**
### **1ï¸âƒ£ Data Ingestion (Azure Data Lake & Azure Function)**
- The raw **Olympics dataset (ZIP file)** is sourced from **Kaggle** via **HTTP** and uploaded to **Azure Data Lake Gen2**.
- A **Blob Trigger in Azure Functions** detects the uploaded ZIP file and **automatically extracts the CSV files** into a designated folder in Data Lake.

### **2ï¸âƒ£ Data Processing & Transformation**
- The **Azure Synapse Pipeline**, orchestrated by **Azure Data Factory**, picks up the extracted CSV files from **Data Lake**.
- **Dataflow transformation** is applied to:
  - Standardize column names.
  - Convert data types.
  - Remove null and duplicate records.
  - Enrich the dataset with calculated metrics (e.g., total medals per country).
- Additional **Apache Spark Notebook transformations** further refine and process the dataset.

### **3ï¸âƒ£ Data Storage (Dedicated SQL Pool - Staging & Target Tables)**
- The cleaned data is first loaded into a **staging table** in **Dedicated SQL Pool**.
- A **Stored Procedure** is executed to move data from the **staging table** to **target tables**, ensuring a structured data model.

### **4ï¸âƒ£ Data Visualization in Power BI**
- Power BI is connected to the **Dedicated SQL Pool** via **DirectQuery**.
- The **Olympics Dashboard** provides insights such as:
  - **Athlete Performance Trends** ğŸ“Š
  - **Country-wise Medal Analysis** ğŸ¥‡ğŸ¥ˆğŸ¥‰
  - **Historical Olympic Trends** ğŸ“…

## ğŸ› ï¸ **Technologies Used**
- **Azure Data Lake Storage (ADLS Gen2)** â€“ Stores raw and processed data.
- **Azure Functions (Blob Trigger)** â€“ Automates file extraction from ZIP.
- **Azure Data Factory (ADF) + Synapse Pipelines** â€“ Orchestrates data processing.
- **Dataflow & Apache Spark Notebooks** â€“ Cleans and transforms the data.
- **Dedicated SQL Pool** â€“ Stores structured data for analysis.
- **Power BI** â€“ Provides interactive visualizations.

## ğŸ¯ **Key Achievements**
âœ… Automated **data ingestion** from Kaggle to Azure Data Lake.  
âœ… Efficient **file extraction** using **Azure Functions (Blob Trigger)**.  
âœ… Performed **scalable data transformation** using Dataflow & Spark.  
âœ… Implemented **SQL-based analytics** with a structured **data model**.  
âœ… Built a **dynamic Power BI dashboard** for **Olympics data insights**. 
